"""
Export Manager for OpenAI Logprobs Application
Handles various export formats for generated text and analysis data.
"""

import json
import csv
import io
from datetime import datetime
from typing import Any, Dict, List
from math import exp

class ExportManager:
    """Manages export functionality for text generation and analysis data."""
    
    def __init__(self):
        """Initialize export manager."""
        pass
    
    def export_text(self, response: Any, original_prompt: str) -> str:
        """
        Export generated text in plain text format.
        
        Args:
            response: OpenAI API response
            original_prompt: Original user prompt
        
        Returns:
            Formatted text string
        """
        if not response:
            return "No text to export"
        
        completed_text = response.choices[0].message.content
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        export_content = f"""OpenAI Text Generation Export
Generated on: {timestamp}

Original Prompt:
{original_prompt}

Generated Text:
{completed_text}

Combined Text:
{original_prompt} {completed_text}

---
Generated by OpenAI Logprobs Text Generator
"""
        return export_content
    
    def export_analysis(self, response: Any, generation_params: Dict[str, Any]) -> str:
        """
        Export detailed analysis in JSON format.
        
        Args:
            response: OpenAI API response
            generation_params: Parameters used for generation
        
        Returns:
            JSON string with analysis data
        """
        if not response or not response.choices[0].logprobs:
            return json.dumps({"error": "No analysis data available"}, indent=2)
        
        tokens = response.choices[0].logprobs.content
        
        # Prepare token analysis
        token_analysis = []
        logprobs = []
        
        for i, token in enumerate(tokens):
            token_str = bytes(token.bytes).decode("utf-8", errors="replace")
            probability = exp(token.logprob) * 100
            logprobs.append(token.logprob)
            
            # Get alternative tokens
            alternatives = []
            if hasattr(token, 'top_logprobs') and token.top_logprobs:
                for alt in token.top_logprobs:
                    alt_str = bytes(alt.bytes).decode("utf-8", errors="replace")
                    alt_prob = exp(alt.logprob) * 100
                    alternatives.append({
                        "token": alt_str,
                        "logprob": alt.logprob,
                        "probability_percent": round(alt_prob, 2)
                    })
            
            token_data = {
                "position": i + 1,
                "token": token_str,
                "token_repr": repr(token_str),
                "logprob": token.logprob,
                "probability_percent": round(probability, 2),
                "confidence_level": "High" if probability > 50 else "Medium" if probability > 20 else "Low",
                "bytes": list(token.bytes),
                "alternatives": alternatives
            }
            token_analysis.append(token_data)
        
        # Calculate statistics
        import numpy as np
        
        perplexity = np.exp(-np.mean(logprobs)) if logprobs else None
        avg_confidence = np.mean([exp(lp) * 100 for lp in logprobs]) if logprobs else None
        uncertainty_score = np.std(logprobs) if logprobs else None
        
        analysis_data = {
            "metadata": {
                "export_timestamp": datetime.now().isoformat(),
                "generation_timestamp": generation_params.get("timestamp"),
                "model": generation_params.get("model"),
                "temperature": generation_params.get("temperature"),
                "max_tokens": generation_params.get("max_tokens")
            },
            "text_content": {
                "generated_text": response.choices[0].message.content,
                "token_count": len(tokens)
            },
            "statistics": {
                "perplexity": round(perplexity, 4) if perplexity else None,
                "average_confidence_percent": round(avg_confidence, 2) if avg_confidence else None,
                "uncertainty_score": round(uncertainty_score, 4) if uncertainty_score else None,
                "min_logprob": min(logprobs) if logprobs else None,
                "max_logprob": max(logprobs) if logprobs else None,
                "mean_logprob": round(np.mean(logprobs), 4) if logprobs else None,
                "std_logprob": round(np.std(logprobs), 4) if logprobs else None
            },
            "token_analysis": token_analysis,
            "model_response": {
                "finish_reason": response.choices[0].finish_reason,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if hasattr(response, 'usage') else None,
                    "completion_tokens": response.usage.completion_tokens if hasattr(response, 'usage') else None,
                    "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else None
                }
            }
        }
        
        return json.dumps(analysis_data, indent=2, ensure_ascii=False)
    
    def export_csv(self, response: Any) -> str:
        """
        Export token analysis in CSV format.
        
        Args:
            response: OpenAI API response
        
        Returns:
            CSV string
        """
        if not response or not response.choices[0].logprobs:
            return "No data available for CSV export"
        
        tokens = response.choices[0].logprobs.content
        
        # Create CSV data
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write header
        headers = [
            "Position",
            "Token",
            "Token_Repr",
            "Raw_Bytes",
            "Logprob",
            "Probability_Percent",
            "Confidence_Level",
            "Top_Alternative_1",
            "Top_Alternative_1_Prob",
            "Top_Alternative_2", 
            "Top_Alternative_2_Prob",
            "Top_Alternative_3",
            "Top_Alternative_3_Prob"
        ]
        writer.writerow(headers)
        
        # Write token data
        for i, token in enumerate(tokens):
            token_str = bytes(token.bytes).decode("utf-8", errors="replace")
            probability = exp(token.logprob) * 100
            confidence_level = "High" if probability > 50 else "Medium" if probability > 20 else "Low"
            
            # Get top alternatives
            alternatives = ["", "", "", "", "", ""]  # 3 alternatives with their probabilities
            if hasattr(token, 'top_logprobs') and token.top_logprobs:
                for j, alt in enumerate(token.top_logprobs[:3]):
                    if j < 3:
                        alt_str = bytes(alt.bytes).decode("utf-8", errors="replace")
                        alt_prob = exp(alt.logprob) * 100
                        alternatives[j*2] = alt_str
                        alternatives[j*2 + 1] = round(alt_prob, 2)
            
            row = [
                i + 1,
                token_str,
                repr(token_str),
                str(list(token.bytes)),
                round(token.logprob, 6),
                round(probability, 2),
                confidence_level
            ] + alternatives
            
            writer.writerow(row)
        
        return output.getvalue()
    
    def export_html_report(self, response: Any, original_prompt: str, generation_params: Dict[str, Any]) -> str:
        """
        Export comprehensive HTML report.
        
        Args:
            response: OpenAI API response
            original_prompt: Original user prompt
            generation_params: Parameters used for generation
        
        Returns:
            HTML string with complete report
        """
        if not response:
            return "<html><body><h1>No data to export</h1></body></html>"
        
        completed_text = response.choices[0].message.content
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Basic statistics
        import numpy as np
        tokens = response.choices[0].logprobs.content if response.choices[0].logprobs else []
        logprobs = [token.logprob for token in tokens]
        
        perplexity = np.exp(-np.mean(logprobs)) if logprobs else "N/A"
        avg_confidence = np.mean([exp(lp) * 100 for lp in logprobs]) if logprobs else "N/A"
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>OpenAI Text Generation Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ text-align: center; border-bottom: 2px solid #333; padding-bottom: 20px; }}
        .section {{ margin: 30px 0; }}
        .stats {{ background-color: #f5f5f5; padding: 20px; border-radius: 8px; }}
        .token {{ display: inline-block; margin: 2px; padding: 4px 8px; border-radius: 4px; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .highlight {{ background-color: #ffffcc; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>OpenAI Text Generation Report</h1>
        <p>Generated on: {timestamp}</p>
    </div>
    
    <div class="section">
        <h2>Generation Parameters</h2>
        <ul>
            <li><strong>Model:</strong> {generation_params.get('model', 'N/A')}</li>
            <li><strong>Temperature:</strong> {generation_params.get('temperature', 'N/A')}</li>
            <li><strong>Max Tokens:</strong> {generation_params.get('max_tokens', 'N/A')}</li>
            <li><strong>Generation Time:</strong> {generation_params.get('timestamp', 'N/A')}</li>
        </ul>
    </div>
    
    <div class="section">
        <h2>Text Content</h2>
        <div class="highlight">
            <h3>Original Prompt:</h3>
            <p><em>{original_prompt}</em></p>
        </div>
        <h3>Generated Text:</h3>
        <p>{completed_text}</p>
    </div>
    
    <div class="section stats">
        <h2>Statistics</h2>
        <ul>
            <li><strong>Token Count:</strong> {len(tokens)}</li>
            <li><strong>Perplexity:</strong> {perplexity:.2f if isinstance(perplexity, float) else perplexity}</li>
            <li><strong>Average Confidence:</strong> {avg_confidence:.1f if isinstance(avg_confidence, float) else avg_confidence}%</li>
        </ul>
    </div>
    
    <div class="section">
        <h2>Token Analysis</h2>
        <table>
            <thead>
                <tr>
                    <th>Position</th>
                    <th>Token</th>
                    <th>Probability (%)</th>
                    <th>Logprob</th>
                    <th>Confidence</th>
                </tr>
            </thead>
            <tbody>
"""
        
        # Add token rows
        for i, token in enumerate(tokens):
            token_str = bytes(token.bytes).decode("utf-8", errors="replace")
            probability = exp(token.logprob) * 100
            confidence = "High" if probability > 50 else "Medium" if probability > 20 else "Low"
            
            html_content += f"""
                <tr>
                    <td>{i + 1}</td>
                    <td>{repr(token_str)}</td>
                    <td>{probability:.2f}</td>
                    <td>{token.logprob:.4f}</td>
                    <td>{confidence}</td>
                </tr>
"""
        
        html_content += """
            </tbody>
        </table>
    </div>
    
    <div class="section">
        <p><em>Report generated by OpenAI Logprobs Text Generator</em></p>
    </div>
</body>
</html>
"""
        
        return html_content
    
    def get_export_formats(self) -> List[str]:
        """
        Get list of available export formats.
        
        Returns:
            List of format names
        """
        return ["text", "json", "csv", "html"]
    
    def get_filename_suggestion(self, format_type: str, prefix: str = "export") -> str:
        """
        Get suggested filename for export.
        
        Args:
            format_type: Export format type
            prefix: Filename prefix
        
        Returns:
            Suggested filename
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        extensions = {
            "text": "txt",
            "json": "json", 
            "csv": "csv",
            "html": "html"
        }
        extension = extensions.get(format_type, "txt")
        return f"{prefix}_{timestamp}.{extension}"
